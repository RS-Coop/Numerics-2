{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oriental-pharmaceutical",
   "metadata": {},
   "source": [
    "# Numerics 2 - HW 2\n",
    "\n",
    "## Cooper Simpson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-security",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subject-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as spl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-sending",
   "metadata": {},
   "source": [
    "## a).\n",
    "\n",
    "We want to show that the Hilbert matrix is positive definite. This matrix is defined as follows:\n",
    "\n",
    "$$ H_{ij} = \\frac{1}{i+j-1} $$\n",
    "\n",
    "So the entries are the unit fractions, it is symmetric, and it has these anti-diagonal bands of the same entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-audience",
   "metadata": {},
   "source": [
    "#### Proof:\n",
    "\n",
    "Let $\\mathbf{H}\\in\\mathbb{R}^{n\\times n}$ be a Hilbert matrix as defined above.\n",
    "\n",
    "We want to show that $\\mathbf{H}$ is positive definite, so take any $\\mathbf{z}\\neq\\mathbf{0}\\in\\mathbb{R}^n$ and we want to show $\\mathbf{z}^T\\mathbf{Hz}>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-madonna",
   "metadata": {},
   "source": [
    "We can write out what this expression means at the element level.\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^T\\mathbf{Hz} = \\sum_{i=1}^n z_i\\cdot(\\mathbf{Hz})_i\n",
    "$$\n",
    "\n",
    "We can then do the same thing for the ith element of the $\\mathbf{Hz}$ term.\n",
    "\n",
    "$$ (\\mathbf{Hz})_i = \\sum_{j=1}^n H_{ij}\\cdot z_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-possession",
   "metadata": {},
   "source": [
    "Putting this together we have the following:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^T\\mathbf{Hz} = \\sum_{i=1}^n z_i\\cdot\\sum_{j=1}^n H_{ij}\\cdot z_j\n",
    "$$\n",
    "\n",
    "Next, we note that $H_{ij}=\\int_0^1x^{i+j-2}dx$, so we replace this in our sum and pull the integral to the outside because the sums are finite.\n",
    "\n",
    "$$ \\implies \\mathbf{z}^T\\mathbf{Hz} = \\int_0^1\\big(\\sum_{i=1}^n z_i\\cdot\\sum_{j=1}^n x^{i+j-2}\\cdot z_j\\big)dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-investor",
   "metadata": {},
   "source": [
    "Splitting up the $x$ term and matching it with its i and j counterparts we can get the following:\n",
    "\n",
    "$$ \\mathbf{z}^T\\mathbf{Hz} = \\int_0^1\\big(\\sum_{i=1}^n z_ix^{i-1}\\cdot\\sum_{j=1}^n z_j\\cdot x^{j-1}\\big)dx = \\int_0^1\\big( \\sum_{k=1}^n z_kx^{k-1} \\big)^2dx$$\n",
    "\n",
    "The sums are of the same form and thus we can write it as the square of one sum. We assumed that $\\mathbf{z}$ was not the zero vector, and thus we can say the following for $x\\in[0,1]$:\n",
    "\n",
    "$$ \\big(\\sum_{k=1}^n z_kx^{k-1}\\big)^2 > 0 \\implies \\int_0^1\\big( \\sum_{k=1}^n z_kx^{k-1} \\big)^2dx > 0 \\implies \\mathbf{z}^T\\mathbf{Hz} > 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-joining",
   "metadata": {},
   "source": [
    "$\\therefore$ the Hilbert matrix $\\mathbf{H}$ is positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-painting",
   "metadata": {},
   "source": [
    "## b).\n",
    "\n",
    "We implement the Power Method for finding the largest eigenvalue. We then apply this to a Hilbert Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mysterious-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute dominant eigenvalue of A\n",
    "#A is complex nxn matrix\n",
    "def eigPower(A, tol=1E-6, maxI=1000):\n",
    "    n = A.shape[0] #Dimension\n",
    "    \n",
    "    q = np.random.randn(n,1) #Initial random vector\n",
    "    l = q.conj().T@A@q #First e-val estimate\n",
    "    \n",
    "    for i in range(maxI):\n",
    "        z = A@q\n",
    "        q = z/np.linalg.norm(z, 2) #Update q\n",
    "        l_new = q.conj().T@A@q #Update e-val\n",
    "        \n",
    "        if np.abs(l_new-l)/np.abs(l_new) < tol:\n",
    "            return l_new\n",
    "            \n",
    "        l = l_new\n",
    "        \n",
    "    raise ValueError('Maximum number of iterations exceeded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-memphis",
   "metadata": {},
   "source": [
    "Let's try a simple toy example to test our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chicken-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.99999802]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0,1],\n",
    "              [-2,-3]]) #Has e-vals -1 and -2\n",
    "\n",
    "e = eigPower(A)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-space",
   "metadata": {},
   "source": [
    "Great! Everything seems to be working fine. Now let's apply this to an order 16 Hilbert matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "patent-scale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant eigenvalue:  1.8600364244729357\n"
     ]
    }
   ],
   "source": [
    "#Define Hilbert matrix\n",
    "n = 16\n",
    "H = spl.hilbert(n)\n",
    "\n",
    "e = eigPower(H)\n",
    "print('Dominant eigenvalue: ',e[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-helena",
   "metadata": {},
   "source": [
    "To check our answer we will also examine the result that Numpy gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "finished-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant eigenvalue (Numpy):  1.8600364427433274\n"
     ]
    }
   ],
   "source": [
    "print('Dominant eigenvalue (Numpy): ', max(np.linalg.eig(H)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-ballot",
   "metadata": {},
   "source": [
    "We see that our method has worked quite well, our dominant eigenvalue is $\\boxed{\\lambda_{max}\\approx1.86}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-stanford",
   "metadata": {},
   "source": [
    "## c).\n",
    "\n",
    "We modify our power method to find the smallest eigenvalue of a Hilbert matrix of size 16. We then investigate the accuracy of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "basic-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute eigenvalue of A closest to mu\n",
    "#A is nxn complex matrix\n",
    "def eigInvPower(A, mu, tol=1E-6, maxI=1000):\n",
    "    n = A.shape[0] #Dimension\n",
    "    \n",
    "    AI = A-mu*np.eye(n) #A-muI\n",
    "    \n",
    "    q = np.random.randn(n,1) #Initial random vector\n",
    "    l = q.conj().T@A@q #First e-val estimate\n",
    "    \n",
    "    for i in range(maxI):\n",
    "        z = np.linalg.solve(AI, q)\n",
    "        \n",
    "        q = z/np.linalg.norm(z, 2) #Update q\n",
    "        l_new = q.conj().T@A@q #Update e-val\n",
    "        \n",
    "        if np.abs(l_new-l)/np.abs(l_new) < tol:\n",
    "            return l_new\n",
    "            \n",
    "        l = l_new\n",
    "        \n",
    "    raise ValueError('Maximum number of iterations exceeded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-values",
   "metadata": {},
   "source": [
    "We will use our Inverse Power method on a Hilber matrix again of size 16. By choosing $\\mu=0$ we will be finding the eigenvalue closest to 0 and thus the smallest eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "thick-border",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest eigenvalue:  -6.064627183294988e-18\n"
     ]
    }
   ],
   "source": [
    "#Define Hilbert matrix\n",
    "n = 16\n",
    "H = spl.hilbert(n)\n",
    "\n",
    "e = eigInvPower(H, 0, maxI=1000000)[0,0]\n",
    "print('Smallest eigenvalue: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-citation",
   "metadata": {},
   "source": [
    "It took a large number of iterations, but eventually our Inverse Power method produces a value on the order of $10^{-18}$ which is within the set tolerence. However, we note that this eigenvalue is negative and we just showed that all Hilbert matrices were positive definite (i.e. have positive eigenvalues). Clearly something is going wrong here numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "clinical-caribbean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest eigenvalue (Numpy):  -6.966678610511967e-18\n"
     ]
    }
   ],
   "source": [
    "e_np = min(np.linalg.eig(H)[0])\n",
    "\n",
    "print('Smallest eigenvalue (Numpy): ', e_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-norman",
   "metadata": {},
   "source": [
    "With Numpy we again see that we are getting a negative value for our eigenvalue which is not possible. To determine the true eigenvalue we can use an exact inverse of our Hilbert matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "known-spectacular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest eigenvalue (Inverse):  (9.197419820651449e-23+0j)\n"
     ]
    }
   ],
   "source": [
    "H_inv = spl.invhilbert(16)\n",
    "e_exact = 1/max(np.linalg.eig(H_inv)[0])\n",
    "\n",
    "print('Smallest eigenvalue (Inverse): ', e_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-preservation",
   "metadata": {},
   "source": [
    "That seems better. At least we are getting a positive value, although it is very small (on the order of $10^{-23}$). Next we look at the error as compared with our calculated eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "effective-machinery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Error:  6.0647191574931944e-18\n",
      "Relative Error:  65939.35338121418\n"
     ]
    }
   ],
   "source": [
    "print('Absolute Error: ', np.abs(e_exact-e))\n",
    "print('Relative Error: ', np.abs(e_exact-e)/np.abs(e_exact))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-wrestling",
   "metadata": {},
   "source": [
    "Well that is an absolutely horrible relative error. We can also see whether or not our approximation is consistent with the following estimate:\n",
    "\n",
    "$$ \\min_{\\lambda\\in\\sigma(H)}|\\lambda-\\mu|\\leq||E||_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-tragedy",
   "metadata": {},
   "source": [
    "In our case $\\mu$ is our approximation, $\\lambda$ is the exact value, and $E$ is the perturbation due to machine imprecision (about $10^{-16}$). Looking at the absolute error above we see that indeed our estimate is within this bound as it is two orders of magnitude smaller. \n",
    "\n",
    "What we have learned is that we need to be careful when trusting numerical output from a computer as it is certainly not always accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-acting",
   "metadata": {},
   "source": [
    "## d)\n",
    "\n",
    "We assume that a real symmetric matrix $\\mathbf{A}$ has eigenvalues such that $\\lambda_1=-\\lambda_2$ and are ordered by their magnitude. To find the eigenvectors $\\mathbf{v}_1$ and $\\mathbf{v}_2$ corresponding to these eigenvalues we suggest a modification of the Power method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-algeria",
   "metadata": {},
   "source": [
    "In short our method is as follows: in the standard Power method let $\\mathbf{q}_1$ be the $2k$ (i.e. even) iterate, and $\\mathbf{q}_2$ be the $2k-1$ (i.e. odd) iterate. Then we have the following: \n",
    "\n",
    "$$\\mathbf{v}_1\\approx\\frac{\\mathbf{q}_1+\\mathbf{q}_2}{2}$$\n",
    "\n",
    "$$\\mathbf{v}_2\\approx\\frac{\\mathbf{q}_1-\\mathbf{q}_2}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-thunder",
   "metadata": {},
   "source": [
    "To see why this follows consider the following form for $\\mathbf{q}^{(k)}$ where $\\mathbf{q}^{(0)} = a_1\\mathbf{x}_1 + \\cdots + a_n\\mathbf{x}_n$ for $\\mathbf{x}_i$ being the columns of the similarity transform for the diagonalization of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-course",
   "metadata": {},
   "source": [
    "$$ \\mathbf{q}^{(k)} = \\mathbf{A}^k\\mathbf{q}^{(0)} = a_1\\lambda_1^k\\big(\\mathbf{x}_1+\\frac{a_2}{a_1}(\\frac{\\lambda_2}{\\lambda_1})^k\\mathbf{x}_2+\\sum_{j=3}^n\\frac{a_j}{a_1}(\\frac{\\lambda_j}{\\lambda_1})^k\\mathbf{x}_j\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-wellington",
   "metadata": {},
   "source": [
    "We can see that the sum term will go to zero as we iterate, so eventually we will be left with something of the form...\n",
    "\n",
    "$$ \\mathbf{q}^{(k)} = \\mathbf{A}^k\\mathbf{q}^{(0)} = a_1\\lambda_1^k\\big(\\mathbf{x}_1+\\frac{a_2}{a_1}(\\frac{\\lambda_2}{\\lambda_1})^k\\mathbf{x}_2\\big) $$\n",
    "\n",
    "Noting that $\\lambda_1=-\\lambda_2$ and distributing we have the following:\n",
    "\n",
    "$$ \\mathbf{q}^{(k)} = \\mathbf{A}^k\\mathbf{q}^{(0)} = a_1\\lambda_1^k\\mathbf{x}_1 + (-1)^ka_2\\lambda_1^k\\mathbf{x}_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-container",
   "metadata": {},
   "source": [
    "Thus we see that for odd $k$ the last term will be negative and for even $k$ it will be positive. This is what we leverage by combining them in our method above to obtain the two eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-pathology",
   "metadata": {},
   "source": [
    "## e).\n",
    "\n",
    "We assume that a real symmetric matrix has an eigenvalue of 1 with multiplicity 8, and the remaining eigenvalues are magnitude $\\leq0.1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-determination",
   "metadata": {},
   "source": [
    "We propose a method for finding an orthogonal basis for the 8 dimensional eigenspace corresponding to this eigenvalue of 1. The core of this approach relies on using the Power method to find eigenvectors corresponding to the largest eigenvalue (1), and then creating an othogonal basis from this. There are a number of ways to approach this, but we will outline the most stable.\n",
    "\n",
    "Take 8 randomly initialized vectors of the same dimension as $\\mathbf{A}$ and form a matrix $\\mathbf{X}$. Apply the power method to this matrix which is essentially applying it to all 8 vectors individually. At each iteration of the Power method after updating the matrix $\\mathbf{X}$, orthogonalize the vectors using Gram-Schmidt, QR, or some other method. Once the matrix $\\mathbf{X}$ has converged to a given tolerance the process is complete. You now have 8 orthogonal vectors that correspond to the eigenvalue 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-manor",
   "metadata": {},
   "source": [
    "We want to determine how many iterations of our process it would take to achieve double precison (i.e. $10^{-16}$) accuracy, so we only need to look at the power method. We can see that the rate of convergence at least $\\mathcal{O}((0.1)^k)$, given the other eigenvalues are smaller than 0.1. Thus we would need about 16 iterations to achieve double precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
